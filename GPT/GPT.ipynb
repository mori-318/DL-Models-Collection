{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86dfa4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129a6310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# デバイスの設定\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf9c965",
   "metadata": {},
   "source": [
    "# GPTモデルの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8eb4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" セルフアテンションを実装 \"\"\"\n",
    "    def __init__(self, config, resid_pdrop=0.1, attn_pdrop=0.1, causal=True) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: Config\n",
    "            resid_pdrop : float\n",
    "                出力projection層のドロップアウト率\n",
    "            attn_pdrop : float\n",
    "                Attentionのドロップアウト率\n",
    "            causal : bool\n",
    "                causal maskを利用するかどうか判別するフラグ\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # 入力をK, Q, Vにそれぞれ変換する全結合層\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "\n",
    "        # Multi-Head Attentionアウトプットの全結合層\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # torch.trilは行列の右上三角部分をゼロにして返す（予測するトークンの右側をマスク）\n",
    "        # nn.Moduleのregister_bufferは, モデルのパラメータとならないtensorを追加するのに使われる\n",
    "        if causal:\n",
    "            self.register_buffer(\n",
    "                name=\"mask\",\n",
    "                tensor=torch.tril(\n",
    "                    torch.ones(config.block_size, config.block_size)\n",
    "                ).view(1, 1, config.block_size, config.block_size)\n",
    "            )\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : torch.Tensor ( b, t, d )\n",
    "                入力ベクトル系列\n",
    "                b : バッチサイズ\n",
    "                t : シークエンス長. コンテクストサイズ (block_size)よりも小さくないといけない\n",
    "                d : Embedding次元数. 上図のd_model\n",
    "        Returns:\n",
    "            y : torch.Tensor ( b, t, d )\n",
    "        \"\"\"\n",
    "        b, t, d = x.size()\n",
    "\n",
    "        # Key, Que, Valueをそれぞれの全結合層で計算\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Multi-Head．d_kやd_vがd_model // n_headsになるような実装だが，必ずしもその必要はない\n",
    "        k = k.view(b, t, self.n_head, d // self.n_head).transpose(1, 2)\n",
    "        q = q.view(b, t, self.n_head, d // self.n_head).transpose(1, 2)\n",
    "        v = v.view(b, t, self.n_head, d // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # QとKの行列積をとり, sqrt(d_k)でスケール\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "\n",
    "        # Causal mask\n",
    "        if hasattr(self, \"mask\"):\n",
    "            att = att.masked_fill(self.mask[:, :, :t, :t] == 0, float('-inf'))\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        # Attention mapとValuesの行列積\n",
    "        y = att @ v\n",
    "\n",
    "        # 各headからの出力を結合\n",
    "        y = y.transpose(1, 2).contiguous().view(b, t, d)\n",
    "\n",
    "        # Attention出力のprojection層\n",
    "        y = self.resid_drop(self.proj(y)) # ( b, t, embd_dim )\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7c4a2",
   "metadata": {},
   "source": [
    "## Blockの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bba31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config, resid_pdrop=0.1, causal=True) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (Config): 設定\n",
    "            resid_pdrop (float, optional): ドロップアウト確率. Defaults to 0.1.\n",
    "            causal (bool, optional): 読み取り方向. Defaults to True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = SelfAttention(config, causal=causal)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor (b, t, d)\n",
    "        \"\"\"\n",
    "        # LinearNormalizationはAttentionやFeedForwardの前に適用する\n",
    "        # プラス、残差接続\n",
    "        x = self.attn(self.ln_1(x)) + x\n",
    "        x = self.mlp(self.ln_2(x)) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6a821",
   "metadata": {},
   "source": [
    "# GPTの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f6b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config, embd_pdrop=0.1) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (Config): 設定\n",
    "            embd_pdrop (float, optional): 埋め込みのドロップアウト確率. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # 文字の表現と位置の表現をつなぐルックアップテーブル\n",
    "        self.tok_embd = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        # Positional encodingで足すベクトルはゼロで初期化し、学習可能なnn.Parameterとして登録する\n",
    "        self.pos_embd = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        # 埋め込みのドロップアウト\n",
    "        self.drop_embd = nn.Dropout(embd_pdrop)\n",
    "        # n_layer個のブロックをnn.Sequentialで連結する\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # デコーダのhead\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        # 引数で与えた関数をモデルのモジュールについて再起的に適用\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: torch.Tensor (b, t)\n",
    "                入力のトークンID\n",
    "            targets: torch.Tensor (b, t)\n",
    "                目標のトークンID\n",
    "        \"\"\"\n",
    "        b, t = idx.shape\n",
    "        # 入力シーケンスがコンテキストサイズを超えていないかチェック\n",
    "        assert t <= self.config.block_size, \"フォワードできません\"\n",
    "\n",
    "        # 文字のidxを表現ベクトルに変換\n",
    "        token_embd = self.tok_embd(idx)  # (b, t, d)\n",
    "        position_embd = self.pos_embd[:, :t, :]  # (1, t, d)\n",
    "        x = self.drop_embd(token_embd + position_embd)\n",
    "\n",
    "        # transformerのブロックを順番に適用\n",
    "        x = self.blocks(x)  # (b, t, d)\n",
    "\n",
    "        # GPT-2で追加された最後のlayer norm\n",
    "        x = self.ln_f(x)  # (b, t, d)\n",
    "\n",
    "        logits = self.head(x)  # (b, t, vocab_size)\n",
    "\n",
    "        # 訓練の時はロスを計算\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" nn.Moduleのクラスメソッドapplyから呼び，moduleに関して再起的に適用 \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        各種パラメータを, AdamWによる更新式にL2正則化項を加えるものと加えないもの\n",
    "        (biases, layer norm / embedding weights)の2グループに分け, 最後に\n",
    "        PyTorchのoptimizerを返している. あまり重要ではない\n",
    "        \"\"\"\n",
    "        decay = set()  # L2正則化をかけるパラメータ\n",
    "        no_decay = set()  # L2正則化をかけないパラメータ\n",
    "        whitelist_weight_modules = (nn.Linear,)  # L2正則化をかけるべきパラメータの型\n",
    "        black_list_weight_modules = (nn.LayerNorm, nn.Embedding)\n",
    "\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # biasは正則化しない\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # L2正則化をかける\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, black_list_weight_modules):\n",
    "                    # L2正則化をかけない\n",
    "                    no_decay.add(fpn)\n",
    "                else:\n",
    "                    # それ以外は正則化をかける\n",
    "                    decay.add(fpn)\n",
    "\n",
    "        # position embeddingのパラメータは正則化しない\n",
    "        no_decay.add('pos_embd')\n",
    "\n",
    "        # 見過ごされたパラメータがないかチェック\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"見過ごされたパラメータがあります\"\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"見落とされたパラメータがあります\"\n",
    "\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2fbbc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
