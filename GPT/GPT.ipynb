{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86dfa4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129a6310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# デバイスの設定\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf9c965",
   "metadata": {},
   "source": [
    "# GPTモデルの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8eb4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" セルフアテンションを実装 \"\"\"\n",
    "    def __init__(self, config, resid_pdrop=0.1, attn_pdrop=0.1, causal=True) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        resid_pdrop : float\n",
    "            出力projection層のドロップアウト率\n",
    "        attn_pdrop : float\n",
    "            Attentionのドロップアウト率\n",
    "        causal : bool\n",
    "            causal maskを利用するかどうか判別するフラグ\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # 入力をK, Q, Vにそれぞれ変換する全結合層\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "\n",
    "        # Multi-Head Attentionアウトプットの全結合層\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # torch.trilは行列の右上三角部分をゼロにして返す（予測するトークンの右側をマスク）\n",
    "        # nn.Moduleのregister_bufferは, モデルのパラメータとならないtensorを追加するのに使われる\n",
    "        if causal:\n",
    "            self.register_buffer(\n",
    "                name=\"mask\",\n",
    "                tensor=torch.tril(\n",
    "                    torch.ones(config.block_size, config.block_size)\n",
    "                ).view(1, 1, config.block_size, config.block_size)\n",
    "            )\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor ( b, t, d )\n",
    "            入力ベクトル系列\n",
    "            b : バッチサイズ\n",
    "            t : シークエンス長. コンテクストサイズ (block_size)よりも小さくないといけない\n",
    "            d : Embedding次元数. 上図のd_model\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        y : torch.Tensor ( b, t, d )\n",
    "        \"\"\"\n",
    "        b, t, d = x.size()\n",
    "\n",
    "        # Key, Que, Valueをそれぞれの全結合層で計算\n",
    "        k = self.key(x)  # ( b, t, d )\n",
    "        q = self.query(x)  # ( b, t, d )\n",
    "        v = self.value(x)  # ( b, t, d )\n",
    "\n",
    "        # Multi-Head．d_kやd_vがd_model // n_headsになるような実装だが，必ずしもその必要はない\n",
    "        k = k.view(b, t, self.n_head, d // self.n_head).transpose(1, 2)  # ( b, n_heads, t, d_k )\n",
    "        q = q.view(b, t, self.n_head, d // self.n_head).transpose(1, 2)  # ( b, n_heads, t, d_k )\n",
    "        v = v.view(b, t, self.n_head, d // self.n_head).transpose(1, 2)  # ( b, n_heads, t, d_v )\n",
    "\n",
    "        # QとKの行列積をとり, sqrt(d_k)でスケール\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # ( b, n_heads, t, d_k ) x ( b, n_heads, d_k, t ) -> ( b, n_heads, t, t )\n",
    "\n",
    "        # Causal mask\n",
    "        if hasattr(self, \"mask\"):\n",
    "            att = att.masked_fill(self.mask[:, :, :t, :t] == 0, float('-inf'))\n",
    "\n",
    "        att = F.softmax(att, dim=-1)  # ( b, n_heads, t, t )\n",
    "\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        # Attention mapとValuesの行列積\n",
    "        y = att @ v\n",
    "        # ( b, n_heads, t, t ) x ( t, n_heads, t, d_v ) -> ( t, n_heads, t, d_v )\n",
    "\n",
    "        # 各headからの出力を結合\n",
    "        y = y.transpose(1, 2).contiguous().view(b, t, d)\n",
    "        # ( b, n_heads, t, d_v ) -> ( b, t, n_heads, d_v ) -> ( b, t, embd_dim )\n",
    "\n",
    "        # Attention出力のprojection層\n",
    "        y = self.resid_drop(self.proj(y)) # ( b, t, embd_dim )\n",
    "\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
